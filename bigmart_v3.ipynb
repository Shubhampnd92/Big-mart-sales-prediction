{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23028b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "# add at top with other imports\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from itertools import combinations\n",
    "from typing import Dict, List, Sequence, Tuple, Optional, Union\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "ArrayLike = Union[np.ndarray, Sequence[np.ndarray], Dict[str, np.ndarray]]\n",
    "\n",
    "# --- CatBoost import (install if missing) ---\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        \"catboost is not installed. Install with:\\n  pip install catboost\"\n",
    "    ) from e\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8830ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# CONFIG\n",
    "# =====================================================================\n",
    "\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    \"\"\"All configurable knobs for the run.\"\"\"\n",
    "    train_path: str = \"train_v9rqX0R.csv\"\n",
    "    test_path: str = \"test_AbJTz2l.csv\"\n",
    "    param_cache_path: str = \"tuned_params.json\"\n",
    "    submission_path: str = \"submission_generic_ensemble.csv\"\n",
    "    seeds: List[int] = None\n",
    "    n_folds: int = 10\n",
    "    tune: bool = True              # re-tune and overwrite cache for selected models\n",
    "    xgb_trials: int = 50\n",
    "    lgb_trials: int = 100\n",
    "    cb_trials: int = 30\n",
    "    early_stopping_rounds: int = 50\n",
    "    models: List[str] = None        # which model keys from registry to use\n",
    "    meta_type: str = \"ridge\"        # \"linear\" or \"ridge\"\n",
    "    needs_scaling: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.seeds is None:\n",
    "            self.seeds = [42, 2021, 7, 1337, 2025] #[42, 2021, 7, 1337, 2025, 999, 111, 8888, 2024, 3333]\n",
    "        if self.models is None:\n",
    "            self.models = [\"xgb\", \"lgb\", \"cb\"] #,\"twlr\"\n",
    "            \n",
    "            \n",
    "# =====================================================================\n",
    "# PARAM CACHING (GENERIC)\n",
    "# =====================================================================\n",
    "\n",
    "def load_cached_params(path: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Load per-model tuned params dictionary; returns {} if not present.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def save_cached_params(path: str, params_by_model: Dict[str, Dict]) -> None:\n",
    "    \"\"\"Persist the dict {model_name: best_params} to JSON.\"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(params_by_model, f, indent=2)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "226f67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# DATA LOADING / PREPROCESSING / FEATURE ENGINEERING\n",
    "# =====================================================================\n",
    "\n",
    "def load_data(cfg: RunConfig) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load train/test CSV and return (train, test, test_id_frame).\"\"\"\n",
    "    train = pd.read_csv(cfg.train_path)\n",
    "    test = pd.read_csv(cfg.test_path)\n",
    "    ids = test[['Item_Identifier', 'Outlet_Identifier']].copy()\n",
    "    return train, test, ids\n",
    "\n",
    "\n",
    "def clean_and_engineer_features(train: pd.DataFrame, test: pd.DataFrame\n",
    "                                ) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Clean + feature engineer. Returns train_final, test_final, feature_cols.\n",
    "    (Logic kept close to your original for comparability.)\n",
    "    \"\"\"\n",
    "    train = train.copy(); test = test.copy()\n",
    "    train['source'] = 'train'; test['source'] = 'test'\n",
    "    data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "    # --- Label fixes ---\n",
    "    data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({\n",
    "        'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'\n",
    "    })\n",
    "\n",
    "    # --- Item_Weight impute (mean per item, else global) ---\n",
    "    item_avg_weight = data.groupby('Item_Identifier')['Item_Weight'].mean()\n",
    "    missing_bool = data['Item_Weight'].isnull()\n",
    "    data.loc[missing_bool, 'Item_Weight'] = data.loc[missing_bool, 'Item_Identifier'].map(item_avg_weight)\n",
    "    data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "    # --- Visibility zero fix (type-wise mean) ---\n",
    "    visibility_avg = data[data['Item_Visibility'] > 0].groupby('Item_Type')['Item_Visibility'].mean()\n",
    "    zero_bool = data['Item_Visibility'] == 0\n",
    "    data.loc[zero_bool, 'Item_Visibility'] = data.loc[zero_bool, 'Item_Type'].map(visibility_avg)\n",
    "    data['Item_Visibility'].fillna(data['Item_Visibility'].mean(), inplace=True)\n",
    "\n",
    "    # --- Impute Outlet_Size by Outlet_Type mode ---\n",
    "    outlet_size_mode = data.groupby('Outlet_Type')['Outlet_Size'].agg(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Medium'\n",
    "    )\n",
    "    missing_bool = data['Outlet_Size'].isnull()\n",
    "    data.loc[missing_bool, 'Outlet_Size'] = data[missing_bool]['Outlet_Type'].map(outlet_size_mode)\n",
    "\n",
    "    # --- Core features ---\n",
    "    data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\n",
    "    data['Item_Type_Combined'] = data['Item_Identifier'].str[:2].map({'FD': 'Food', 'NC': 'Non-Consumable', 'DR': 'Drinks'})\n",
    "    data.loc[data['Item_Type_Combined'] == 'Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'\n",
    "\n",
    "    # --- Extra features ---\n",
    "    data['Item_MRP_Bins'] = pd.cut(data['Item_MRP'], bins=4, labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "    data['Item_Visibility_Bins'] = pd.qcut(\n",
    "        data['Item_Visibility'], q=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High']\n",
    "    )\n",
    "    data['Price_Per_Weight'] = data['Item_MRP'] / (data['Item_Weight'] + 0.01)\n",
    "    data['Visibility_MRP_Ratio'] = data['Item_Visibility'] / data['Item_MRP']\n",
    "\n",
    "    # --- Aggregations ---\n",
    "    store_item_count = data.groupby('Outlet_Identifier')['Item_Identifier'].count().to_dict()\n",
    "    data['Store_Item_Count'] = data['Outlet_Identifier'].map(store_item_count)\n",
    "\n",
    "    mean_sales_by_store = train.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean().to_dict()\n",
    "    data['Store_Avg_Sales'] = data['Outlet_Identifier'].map(mean_sales_by_store)\n",
    "    data['Store_Avg_Sales'].fillna(data['Store_Avg_Sales'].mean(), inplace=True)\n",
    "\n",
    "    # --- Ordinals ---\n",
    "    data['Outlet_Type_Num'] = data['Outlet_Type'].map({'Grocery Store': 0, 'Supermarket Type1': 1, 'Supermarket Type2': 2, 'Supermarket Type3': 3})\n",
    "    data['Outlet_Location_Type_Num'] = data['Outlet_Location_Type'].map({'Tier 3': 0, 'Tier 2': 1, 'Tier 1': 2})\n",
    "    data['Outlet_Size_Num'] = data['Outlet_Size'].map({'Small': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "    # --- Encodings ---\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    cat_cols = ['Item_Fat_Content', 'Item_Type', 'Item_Type_Combined', 'Item_MRP_Bins', 'Item_Visibility_Bins']\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        data[col + '_Encoded'] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "    # --- One-hot outlets ---\n",
    "    outlet_dummies = pd.get_dummies(data['Outlet_Identifier'], prefix='Outlet')\n",
    "    data = pd.concat([data, outlet_dummies], axis=1)\n",
    "\n",
    "    # --- Item frequency ---\n",
    "    item_counts = data['Item_Identifier'].value_counts().to_dict()\n",
    "    data['Item_Count'] = data['Item_Identifier'].map(item_counts)\n",
    "\n",
    "    # --- Final splits ---\n",
    "    train_final = data[data['source'] == 'train'].copy()\n",
    "    test_final = data[data['source'] == 'test'].copy()\n",
    "\n",
    "    feature_cols = [\n",
    "        'Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Years',\n",
    "        'Price_Per_Weight', 'Visibility_MRP_Ratio', 'Store_Item_Count', \n",
    "        'Store_Avg_Sales', 'Item_Count',\n",
    "        'Outlet_Type_Num', 'Outlet_Location_Type_Num', 'Outlet_Size_Num',\n",
    "        'Item_Fat_Content_Encoded', 'Item_Type_Encoded', \n",
    "        'Item_Type_Combined_Encoded', 'Item_MRP_Bins_Encoded', \n",
    "        'Item_Visibility_Bins_Encoded'\n",
    "    ]\n",
    "    outlet_cols = [c for c in data.columns if c.startswith('Outlet_OUT')]\n",
    "    feature_cols.extend(outlet_cols)\n",
    "\n",
    "    return train_final, test_final, feature_cols\n",
    "\n",
    "# def clean_and_engineer_features(train: pd.DataFrame, test: pd.DataFrame\n",
    "#                                 ) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "#     \"\"\"\n",
    "#     Clean + feature engineer. Returns train_final, test_final, feature_cols.\n",
    "#     (Logic kept close to your original for comparability.)\n",
    "#     \"\"\"\n",
    "#     train = train.copy(); test = test.copy()\n",
    "#     train['source'] = 'train'; test['source'] = 'test'\n",
    "#     data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "#     # --- Label fixes ---\n",
    "#     data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({\n",
    "#         'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'\n",
    "#     })\n",
    "\n",
    "#     # --- Item_Weight impute (mean per item, else global) ---\n",
    "#     item_avg_weight = data.groupby('Item_Identifier')['Item_Weight'].mean()\n",
    "#     missing_bool = data['Item_Weight'].isnull()\n",
    "#     data.loc[missing_bool, 'Item_Weight'] = data.loc[missing_bool, 'Item_Identifier'].map(item_avg_weight)\n",
    "#     data['Item_Weight'].fillna(data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "#     # --- Visibility zero fix (type-wise mean) ---\n",
    "#     visibility_avg = data[data['Item_Visibility'] > 0].groupby('Item_Type')['Item_Visibility'].mean()\n",
    "#     zero_bool = data['Item_Visibility'] == 0\n",
    "#     data.loc[zero_bool, 'Item_Visibility'] = data.loc[zero_bool, 'Item_Type'].map(visibility_avg)\n",
    "#     data['Item_Visibility'].fillna(data['Item_Visibility'].mean(), inplace=True)\n",
    "\n",
    "#     # --- Impute Outlet_Size by Outlet_Type mode ---\n",
    "#     outlet_size_mode = data.groupby('Outlet_Type')['Outlet_Size'].agg(\n",
    "#         lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Medium'\n",
    "#     )\n",
    "#     missing_bool = data['Outlet_Size'].isnull()\n",
    "#     data.loc[missing_bool, 'Outlet_Size'] = data[missing_bool]['Outlet_Type'].map(outlet_size_mode)\n",
    "\n",
    "#     # --- Core features ---\n",
    "#     data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\n",
    "#     data['Item_Type_Combined'] = data['Item_Identifier'].str[:2].map({'FD': 'Food', 'NC': 'Non-Consumable', 'DR': 'Drinks'})\n",
    "#     data.loc[data['Item_Type_Combined'] == 'Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'\n",
    "\n",
    "#     # --- Extra features ---\n",
    "#     import numpy as np\n",
    "#     data['Item_MRP_Bins'] = pd.cut(data['Item_MRP'], bins=4, labels=['Low', 'Medium', 'High', 'Very_High'])\n",
    "#     data['Item_Visibility_Bins'] = pd.qcut(\n",
    "#         data['Item_Visibility'], q=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High']\n",
    "#     )\n",
    "#     data['Price_Per_Weight'] = data['Item_MRP'] / (data['Item_Weight'] + 0.01)\n",
    "#     data['Visibility_MRP_Ratio'] = data['Item_Visibility'] / data['Item_MRP']\n",
    "#     # NEW: log transforms to reduce skew\n",
    "#     #data['Log_Item_MRP'] = np.log1p(data['Item_MRP'])                 # NEW\n",
    "#     #data['Log_Item_Visibility'] = np.log1p(data['Item_Visibility'])   # NEW\n",
    "#     #data['Log_Price_Per_Weight'] = np.log1p(data['Price_Per_Weight']) # NEW\n",
    "\n",
    "#     # --- Store- and category-aware counts (less collinear than raw totals) ---\n",
    "#     outlet_total_items = data.groupby('Outlet_Identifier')['Item_Identifier'].transform('count')\n",
    "#     outlet_cat_items = data.groupby(['Outlet_Identifier', 'Item_Type_Combined'])['Item_Identifier'].transform('count')\n",
    "#     data['Outlet_Cat_Share'] = (outlet_cat_items / (outlet_total_items.replace(0, np.nan))).fillna(0)  # NEW\n",
    "#     data['Outlet_Cat_Unique_Count'] = data.groupby('Outlet_Identifier')['Item_Type_Combined'].transform('nunique')  # NEW\n",
    "\n",
    "#     # --- Relative pricing & visibility (within outlet) ---\n",
    "#     outlet_cat_median_mrp = data.groupby(['Outlet_Identifier', 'Item_Type_Combined'])['Item_MRP'].transform('median')\n",
    "#     data['MRP_Relative_to_Outlet'] = (data['Item_MRP'] - outlet_cat_median_mrp) / (outlet_cat_median_mrp + 1e-6)  # NEW\n",
    "\n",
    "#     vis_median_by_outlet = data.loc[data['Item_Visibility'] > 0].groupby('Outlet_Identifier')['Item_Visibility'].median()\n",
    "#     data['Outlet_Vis_Median'] = data['Outlet_Identifier'].map(vis_median_by_outlet)  # helper (not used as feature)\n",
    "#     data['Outlet_Vis_Median'].fillna(data['Item_Visibility'].median(), inplace=True)\n",
    "#     data['Visibility_Relative_to_Outlet'] = (data['Item_Visibility'] - data['Outlet_Vis_Median']) / (data['Outlet_Vis_Median'] + 1e-6)  # NEW\n",
    "\n",
    "#     # --- Aggregations (keep non-target based; avoid leakage) ---\n",
    "#     store_item_count = data.groupby('Outlet_Identifier')['Item_Identifier'].count().to_dict()\n",
    "#     data['Store_Item_Count'] = data['Outlet_Identifier'].map(store_item_count)\n",
    "\n",
    "#     # Historical sales encodings with leakage control (Leave-One-Out + smoothing)\n",
    "#     global_mean = train['Item_Outlet_Sales'].mean()\n",
    "#     prior = 5.0  # smoothing strength\n",
    "\n",
    "#     # Use engineered TRAIN slice for group stats to avoid KeyError on engineered cols\n",
    "#     train_ext = data[data['source'] == 'train']\n",
    "\n",
    "#     def add_mean_encoding(group_col: str, new_col: str):\n",
    "#         sums = train_ext.groupby(group_col)['Item_Outlet_Sales'].sum()\n",
    "#         cnts = train_ext.groupby(group_col)['Item_Outlet_Sales'].count()\n",
    "#         gsum = data[group_col].map(sums)\n",
    "#         gcnt = data[group_col].map(cnts)\n",
    "#         enc = pd.Series(index=data.index, dtype='float64')\n",
    "#         tr_idx = data['source'] == 'train'\n",
    "#         te_idx = data['source'] == 'test'\n",
    "#         y_tr = data.loc[tr_idx, 'Item_Outlet_Sales']\n",
    "#         enc.loc[tr_idx] = ((gsum.loc[tr_idx] - y_tr) + prior * global_mean) / (((gcnt.loc[tr_idx] - 1).clip(lower=0)) + prior)\n",
    "#         enc.loc[te_idx] = ((gsum.loc[te_idx]) + prior * global_mean) / ((gcnt.loc[te_idx]) + prior)\n",
    "#         data[new_col] = enc.fillna(global_mean)\n",
    "\n",
    "#     # Powerful historical demand signals (item / outlet / category)\n",
    "#     #add_mean_encoding('Item_Identifier', 'Enc_Item_Mean_Sales')          # NEW: target-mean encoding per item\n",
    "#     #add_mean_encoding('Outlet_Identifier', 'Enc_Outlet_Mean_Sales')      # NEW: target-mean encoding per outlet\n",
    "#     #add_mean_encoding('Item_Type_Combined', 'Enc_Cat_Mean_Sales')        # NEW: target-mean encoding per category\n",
    "\n",
    "#     # (Legacy) Mean sales by store for completeness (not used in features to avoid leakage risk)\n",
    "#     mean_sales_by_store = train.groupby('Outlet_Identifier')['Item_Outlet_Sales'].mean().to_dict()\n",
    "#     data['Store_Avg_Sales'] = data['Outlet_Identifier'].map(mean_sales_by_store)\n",
    "#     data['Store_Avg_Sales'].fillna(data['Store_Avg_Sales'].mean(), inplace=True)\n",
    "\n",
    "#     # --- Ordinals ---\n",
    "#     data['Outlet_Type_Num'] = data['Outlet_Type'].map({'Grocery Store': 0, 'Supermarket Type1': 1, 'Supermarket Type2': 2, 'Supermarket Type3': 3})\n",
    "#     data['Outlet_Location_Type_Num'] = data['Outlet_Location_Type'].map({'Tier 3': 0, 'Tier 2': 1, 'Tier 1': 2})\n",
    "#     data['Outlet_Size_Num'] = data['Outlet_Size'].map({'Small': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "#     # --- Encodings ---\n",
    "#     from sklearn.preprocessing import LabelEncoder\n",
    "#     cat_cols = ['Item_Fat_Content', 'Item_Type', 'Item_Type_Combined', 'Item_MRP_Bins', 'Item_Visibility_Bins']\n",
    "#     for col in cat_cols:\n",
    "#         le = LabelEncoder()\n",
    "#         data[col + '_Encoded'] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "#     # --- One-hot outlets ---\n",
    "#     outlet_dummies = pd.get_dummies(data['Outlet_Identifier'], prefix='Outlet')\n",
    "#     data = pd.concat([data, outlet_dummies], axis=1)\n",
    "\n",
    "#     # --- Item frequency ---\n",
    "#     item_counts = data['Item_Identifier'].value_counts().to_dict()\n",
    "#     data['Item_Count'] = data['Item_Identifier'].map(item_counts)\n",
    "\n",
    "#     # --- Final splits ---\n",
    "#     train_final = data[data['source'] == 'train'].copy()\n",
    "#     test_final = data[data['source'] == 'test'].copy()\n",
    "\n",
    "#     # --- Feature columns (AUTO SELECT + optional drop) ---\n",
    "#     # Build from all numeric columns, then remove targets/helpers/known leakage.\n",
    "#     non_feature_cols = {\n",
    "#         'Item_Outlet_Sales',   # target (train only)\n",
    "#         'Outlet_Vis_Median',   # helper column\n",
    "#         #'Store_Avg_Sales',     # legacy mean target by store (leak-prone)\n",
    "#         'Outlet_Establishment_Year'  # superseded by Outlet_Years\n",
    "#     }\n",
    "#     numeric_cols = [c for c in data.select_dtypes(include=[np.number]).columns]\n",
    "#     feature_cols = [c for c in numeric_cols if c not in non_feature_cols]\n",
    "\n",
    "#     # Optional user-defined drops via DataFrame attrs (no signature change)\n",
    "#     user_drop = set()\n",
    "#     if hasattr(train, 'attrs') and isinstance(train.attrs.get('drop_features', None), list):\n",
    "#         user_drop |= set(train.attrs['drop_features'])\n",
    "#     if hasattr(test, 'attrs') and isinstance(test.attrs.get('drop_features', None), list):\n",
    "#         user_drop |= set(test.attrs['drop_features'])\n",
    "#     if user_drop:\n",
    "#         feature_cols = [c for c in feature_cols if c not in user_drop]\n",
    "        \n",
    "#     print(len(feature_cols), \":::\",feature_cols)\n",
    "#     return train_final, test_final, feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d3710e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# MODEL SPEC INTERFACE (GENERIC)\n",
    "# =====================================================================\n",
    "\n",
    "class ModelSpec:\n",
    "    \"\"\"\n",
    "    Describes how to tune, build, fit, and predict with a base model.\n",
    "    Adding a model = implement 3 small functions + register it here.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        tuner: Callable[[pd.DataFrame, pd.Series, \"RunConfig\"], Dict],\n",
    "        builder: Callable[[Dict, int], object],\n",
    "        fit_fn: Callable[[object, pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, \"RunConfig\"], None],\n",
    "        predict_fn: Callable[[object, pd.DataFrame], np.ndarray] = None,\n",
    "        needs_scaling: bool = False,   # set True for models that require scaling\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.tuner = tuner\n",
    "        self.builder = builder\n",
    "        self.fit_fn = fit_fn\n",
    "        self.predict_fn = predict_fn or (lambda model, X: model.predict(X))\n",
    "        self.needs_scaling = needs_scaling\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# TUNERS (RE-USED ACROSS MODELS) — NO GLOBAL SCALING\n",
    "# =====================================================================\n",
    "\n",
    "def tune_xgb(X: pd.DataFrame, y: pd.Series, cfg: RunConfig) -> Dict:\n",
    "    \"\"\"Optuna tuner for XGBoost (Tweedie).\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': trial.suggest_categorical('objective', ['reg:tweedie']),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'random_state': 42, 'verbosity': 0\n",
    "        }\n",
    "        if params['objective'] == 'reg:tweedie':\n",
    "            params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1.1, 1.9)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for tr, va in kf.split(X):\n",
    "            X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "            y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "            m = xgb.XGBRegressor(**params)\n",
    "            m.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n",
    "                  early_stopping_rounds=cfg.early_stopping_rounds, verbose=False)\n",
    "            p = m.predict(X_va)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_va, p)))\n",
    "        return float(np.mean(scores))\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=cfg.xgb_trials, show_progress_bar=True)\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "def tune_lgb(X: pd.DataFrame, y: pd.Series, cfg: RunConfig) -> Dict:\n",
    "    \"\"\"Optuna tuner for LightGBM (regression/Tweedie).\"\"\"\n",
    "    def objective(trial):\n",
    "        objective = trial.suggest_categorical('objective', ['regression', 'tweedie'])\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'objective': objective, 'metric': 'rmse',\n",
    "            'random_state': 42, 'verbosity': -1\n",
    "        }\n",
    "        if objective == 'tweedie':\n",
    "            params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1.1, 1.9)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for tr, va in kf.split(X):\n",
    "            X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "            y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "            m = lgb.LGBMRegressor(**params)\n",
    "            m.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n",
    "                  callbacks=[lgb.early_stopping(cfg.early_stopping_rounds), lgb.log_evaluation(0)])\n",
    "            p = m.predict(X_va)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_va, p)))\n",
    "        return float(np.mean(scores))\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=cfg.lgb_trials, show_progress_bar=True)\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "def tune_cb(X: pd.DataFrame, y: pd.Series, cfg: RunConfig) -> Dict:\n",
    "    \"\"\"Optuna tuner for CatBoost (RMSE/Tweedie).\"\"\"\n",
    "    def objective(trial):\n",
    "        bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS'])\n",
    "        use_tweedie = trial.suggest_categorical('use_tweedie', [True])\n",
    "        if use_tweedie:\n",
    "            vp = trial.suggest_float('tweedie_variance_power', 1.1, 1.9)\n",
    "            loss = f'Tweedie:variance_power={vp}'\n",
    "        else:\n",
    "            loss = 'RMSE'\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 500, 3000),\n",
    "            'depth': trial.suggest_int('depth', 4, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),\n",
    "            'rsm': trial.suggest_float('rsm', 0.5, 1.0),\n",
    "            'bootstrap_type': bootstrap_type,\n",
    "            'loss_function': loss, 'eval_metric': 'RMSE',\n",
    "            'random_state': 42, 'verbose': False, 'allow_writing_files': False, 'task_type': 'CPU'\n",
    "        }\n",
    "        if bootstrap_type == 'Bayesian':\n",
    "            params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 5.0)\n",
    "        elif bootstrap_type == 'Bernoulli':\n",
    "            params['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for tr, va in kf.split(X):\n",
    "            X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "            y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "            m = CatBoostRegressor(**params)\n",
    "            m.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "            p = m.predict(X_va)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_va, p)))\n",
    "        return float(np.mean(scores))\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=cfg.cb_trials, show_progress_bar=True)\n",
    "    best = study.best_params\n",
    "    # normalize Tweedie flags for reuse\n",
    "    if best.get('use_tweedie', False):\n",
    "        vp = best.get('tweedie_variance_power', 1.5)\n",
    "        best['loss_function'] = f'Tweedie:variance_power={vp}'\n",
    "    else:\n",
    "        best['loss_function'] = 'RMSE'\n",
    "    best.pop('use_tweedie', None)\n",
    "    best.pop('tweedie_variance_power', None)\n",
    "    return best\n",
    "\n",
    "def tune_twlr(X: pd.DataFrame, y: pd.Series, cfg: RunConfig) -> Dict:\n",
    "    \"\"\"Optuna tuner for Tweedie GLM (linear regression).\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            # power in (1,2): compound Poisson-Gamma; matches your GBM Tweedie range\n",
    "            'power': trial.suggest_float('power', 1.1, 1.9),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "            #'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "            #'max_iter': trial.suggest_int('max_iter', 200, 2000),\n",
    "            #'tol': trial.suggest_float('tol', 1e-8, 1e-3, log=True),\n",
    "            'link': 'auto',         # uses log link for 1<p<2\n",
    "            # solver is lbfgs for sklearn>=1.2; leave default\n",
    "        }\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        for tr, va in kf.split(X):\n",
    "            X_tr, X_va = X.iloc[tr], X.iloc[va]\n",
    "            y_tr, y_va = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "            # Tweedie (1<p<2) requires y >= 0\n",
    "            y_tr = np.clip(y_tr, 0, None)\n",
    "            y_va = np.clip(y_va, 0, None)\n",
    "\n",
    "            m = TweedieRegressor(**params)\n",
    "            m.fit(X_tr, y_tr)\n",
    "            p = m.predict(X_va)\n",
    "            scores.append(np.sqrt(mean_squared_error(y_va, p)))\n",
    "\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "    n_trials = int(getattr(cfg, \"twlr_trials\", 15))  # falls back to 80 if not in RunConfig\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    return study.best_params\n",
    "\n",
    "# =====================================================================\n",
    "# BUILDERS + FITTERS (PER-MODEL)\n",
    "# =====================================================================\n",
    "\n",
    "def build_xgb(params: Dict, seed: int):\n",
    "    return xgb.XGBRegressor(**{**params, 'random_state': seed, 'verbosity': 0})\n",
    "\n",
    "def fit_xgb(model, X_tr, y_tr, X_va, y_va, cfg: RunConfig):\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n",
    "              early_stopping_rounds=cfg.early_stopping_rounds, verbose=False)\n",
    "\n",
    "def build_lgb(params: Dict, seed: int):\n",
    "    merged = {**params, 'random_state': seed, 'verbosity': -1,\n",
    "              'bagging_seed': seed, 'feature_fraction_seed': seed}\n",
    "    return lgb.LGBMRegressor(**merged)\n",
    "\n",
    "def fit_lgb(model, X_tr, y_tr, X_va, y_va, cfg: RunConfig):\n",
    "    model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n",
    "              callbacks=[lgb.early_stopping(cfg.early_stopping_rounds), lgb.log_evaluation(0)])\n",
    "\n",
    "def build_cb(params: Dict, seed: int):\n",
    "    merged = {**params, 'eval_metric': 'RMSE', 'random_state': seed,\n",
    "              'thread_count': -1, 'verbose': False, 'allow_writing_files': False}\n",
    "    return CatBoostRegressor(**merged)\n",
    "\n",
    "def fit_cb(model, X_tr, y_tr, X_va, y_va, cfg: RunConfig):\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "\n",
    "def build_twlr(params: Dict, seed: int):\n",
    "    # seed not used by TweedieRegressor (deterministic optimizer)\n",
    "    return TweedieRegressor(**params)\n",
    "\n",
    "def fit_twlr(model, X_tr, y_tr, X_va, y_va, cfg: RunConfig):\n",
    "    # Outer pipeline should handle scaling because needs_scaling=True below.\n",
    "    # Ensure non-negative targets for Tweedie.\n",
    "    y_tr = np.clip(y_tr, 0, None)\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "# =====================================================================\n",
    "# MODEL REGISTRY (ADD NEW MODELS HERE)\n",
    "# =====================================================================\n",
    "\n",
    "MODEL_REGISTRY: Dict[str, ModelSpec] = {\n",
    "    \"xgb\": ModelSpec(\"xgb\", tuner=tune_xgb, builder=build_xgb, fit_fn=fit_xgb, needs_scaling=False),\n",
    "    \"lgb\": ModelSpec(\"lgb\", tuner=tune_lgb, builder=build_lgb, fit_fn=fit_lgb, needs_scaling=False),\n",
    "    \"cb\":  ModelSpec(\"cb\",  tuner=tune_cb,  builder=build_cb,  fit_fn=fit_cb,  needs_scaling=False),\n",
    "    \"twlr\" : ModelSpec(\"twlr\",tuner=tune_twlr,builder=build_twlr,fit_fn=fit_twlr,needs_scaling=False)\n",
    "}\n",
    "# To add a new model (example sketch):\n",
    "# def tune_enet(X, y, cfg): ...  # inside CV, fit a scaler on X_tr, transform X_tr/X_va\n",
    "# def build_enet(params, seed): return ElasticNet(**params, random_state=seed)\n",
    "# def fit_enet(model, X_tr, y_tr, X_va, y_va, cfg): model.fit(X_tr, y_tr)\n",
    "# MODEL_REGISTRY[\"enet\"] = ModelSpec(\"enet\", tune_enet, build_enet, fit_enet, needs_scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57ac9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# META-LEARNER (GENERIC, RAW PREDICTIONS ONLY)\n",
    "# =====================================================================\n",
    "\n",
    "def create_meta_features(\n",
    "    preds: ArrayLike,\n",
    "    *,\n",
    "    order: Optional[List[str]] = None,     # fixed model order (for dict input)\n",
    "    include_raw: bool = True,\n",
    "    include_squares: bool = True,\n",
    "    include_pairwise: bool = True,         # pairwise interactions: xi * xj\n",
    "    include_stats: Sequence[str] = (\"max\", \"min\", \"mean\", \"std\")\n",
    ") -> Tuple[np.ndarray, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Build meta-features from an arbitrary number of base-model predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : dict[str, 1D array] | list/tuple[1D array] | 2D array (n_samples, n_models)\n",
    "        Base predictions (OOF or test). All arrays must have the same length.\n",
    "    order : list[str], optional\n",
    "        If preds is a dict, enforce this column order (keeps train/test consistent).\n",
    "        If None, dict keys are sorted alphabetically.\n",
    "    include_raw : bool\n",
    "        Include raw base predictions as features.\n",
    "    include_squares : bool\n",
    "        Include squared terms for each base prediction.\n",
    "    include_pairwise : bool\n",
    "        Include pairwise interaction terms for each unordered pair (i<j).\n",
    "    include_stats : sequence of {\"max\",\"min\",\"mean\",\"std\"}\n",
    "        Include row-wise reduction stats across models.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Xf : ndarray, shape (n_samples, n_features)\n",
    "        Engineered meta-features.\n",
    "    feature_names : list[str]\n",
    "        Names/descriptors for each column in Xf.\n",
    "    model_names : list[str]\n",
    "        The ordered model names corresponding to input columns (to reuse later).\n",
    "    \"\"\"\n",
    "    # ---- normalize input to matrix (n_samples, n_models) + names ----\n",
    "    if isinstance(preds, dict):\n",
    "        model_names = list(order) if order is not None else sorted(preds.keys())\n",
    "        cols = [np.asarray(preds[name]).reshape(-1) for name in model_names]\n",
    "    elif isinstance(preds, (list, tuple)):\n",
    "        cols = [np.asarray(a).reshape(-1) for a in preds]\n",
    "        model_names = [f\"m{i}\" for i in range(len(cols))]\n",
    "    else:\n",
    "        X = np.asarray(preds)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        model_names = [f\"m{i}\" for i in range(X.shape[1])]\n",
    "        cols = [X[:, i] for i in range(X.shape[1])]\n",
    "\n",
    "    # length consistency check\n",
    "    if len(cols) == 0:\n",
    "        raise ValueError(\"No prediction columns provided.\")\n",
    "    n = len(cols[0])\n",
    "    if any(len(c) != n for c in cols):\n",
    "        raise ValueError(\"All prediction arrays must have the same length.\")\n",
    "\n",
    "    X = np.column_stack(cols).astype(np.float64, copy=False)\n",
    "    m = X.shape[1]\n",
    "\n",
    "    features: List[np.ndarray] = []\n",
    "    names: List[str] = []\n",
    "\n",
    "    # ---- raw ----\n",
    "    if include_raw:\n",
    "        features.append(X)\n",
    "        names += [f\"{name}\" for name in model_names]\n",
    "\n",
    "    # ---- squares ----\n",
    "    if include_squares:\n",
    "        features.append(X ** 2)\n",
    "        names += [f\"{name}^2\" for name in model_names]\n",
    "\n",
    "    # ---- pairwise interactions ----\n",
    "    if include_pairwise and m >= 2:\n",
    "        for i, j in combinations(range(m), 2):\n",
    "            features.append((X[:, i] * X[:, j]).reshape(-1, 1))\n",
    "            names.append(f\"{model_names[i]}*{model_names[j]}\")\n",
    "\n",
    "    # ---- row-wise stats ----\n",
    "    stats = set((include_stats or ()))\n",
    "    stats = {s.lower() for s in stats}\n",
    "    if \"max\" in stats:\n",
    "        features.append(np.max(X, axis=1, keepdims=True)); names.append(\"row_max\")\n",
    "    if \"min\" in stats:\n",
    "        features.append(np.min(X, axis=1, keepdims=True)); names.append(\"row_min\")\n",
    "    if \"mean\" in stats:\n",
    "        features.append(np.mean(X, axis=1, keepdims=True)); names.append(\"row_mean\")\n",
    "    if \"std\" in stats:\n",
    "        features.append(np.std(X, axis=1, keepdims=True)); names.append(\"row_std\")\n",
    "\n",
    "    Xf = np.column_stack(features) if features else np.empty((n, 0))\n",
    "    return Xf, names, model_names\n",
    "\n",
    "\n",
    "def create_meta_matrix(\n",
    "    preds_by_model: Dict[str, np.ndarray],\n",
    "    order: Optional[List[str]] = None,\n",
    "    engineered: bool = False,\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper for dict inputs.\n",
    "\n",
    "    engineered=False  -> raw only\n",
    "    engineered=True   -> raw + squares + pairwise + row_max/min/std\n",
    "    \"\"\"\n",
    "    if not engineered:\n",
    "        X_raw, _, model_order = create_meta_features(\n",
    "            preds_by_model,\n",
    "            order=order,\n",
    "            include_raw=True,\n",
    "            include_squares=False,\n",
    "            include_pairwise=False,\n",
    "            include_stats=(),\n",
    "        )\n",
    "        return X_raw, model_order\n",
    "\n",
    "    X_eng, _, model_order = create_meta_features(\n",
    "        preds_by_model,\n",
    "        order=order,\n",
    "        include_raw=True,\n",
    "        include_squares=True,\n",
    "        include_pairwise=True,\n",
    "        include_stats=(\"max\", \"min\", \"std\"),\n",
    "    )\n",
    "    return X_eng, model_order\n",
    "\n",
    "\n",
    "def fit_meta(\n",
    "    oof_by_model: Dict[str, np.ndarray],\n",
    "    y_true: np.ndarray,\n",
    "    meta_type: str,\n",
    "    *,\n",
    "    engineered: bool = False\n",
    ") -> Tuple[object, List[str], float]:\n",
    "    \"\"\"\n",
    "    Fit a meta-learner on arbitrary N base predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model        : fitted meta model (LinearRegression or Ridge)\n",
    "    base_order   : list[str]  (order of base models used for columns)\n",
    "    oof_rmse     : float       RMSE on the OOF meta-fit\n",
    "    \"\"\"\n",
    "    # Build meta matrix (raw by default; engineered if requested)\n",
    "    P, base_order = create_meta_matrix(oof_by_model, order=None, engineered=engineered)\n",
    "\n",
    "    if meta_type == \"ridge\":\n",
    "        alphas = [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0]\n",
    "        gs = GridSearchCV(Ridge(fit_intercept=False), {'alpha': alphas}, cv=5,\n",
    "                          scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        gs.fit(P, y_true)\n",
    "        model = gs.best_estimator_\n",
    "    elif meta_type == \"linear\":\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(P, y_true)\n",
    "    else:\n",
    "        raise ValueError(\"meta_type must be 'linear' or 'ridge'\")\n",
    "\n",
    "    oof_pred = np.maximum(model.predict(P), 0)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, oof_pred)))\n",
    "    return model, base_order, rmse\n",
    "\n",
    "\n",
    "def predict_meta(\n",
    "    model: object,\n",
    "    col_order: List[str],\n",
    "    preds_by_model: Dict[str, np.ndarray],\n",
    "    *,\n",
    "    engineered: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict with the meta-learner using the same base column order and feature mode.\n",
    "    \"\"\"\n",
    "    P_test, _ = create_meta_matrix(preds_by_model, order=col_order, engineered=engineered)\n",
    "    return np.maximum(model.predict(P_test), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e249e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# TUNING OR LOADING PARAMS (ONLY FOR SELECTED MODELS)\n",
    "# =====================================================================\n",
    "\n",
    "def get_params_for_models(cfg: RunConfig, X_raw: pd.DataFrame, y: pd.Series\n",
    "                          ) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load cache; for selected models, ensure tuned params exist (tune if missing or --tune).\n",
    "    Saves back to cache and returns {model_name: params}.\n",
    "    \"\"\"\n",
    "    cache = load_cached_params(cfg.param_cache_path)\n",
    "    changed = False\n",
    "    out: Dict[str, Dict] = {}\n",
    "\n",
    "    for key in cfg.models:\n",
    "        spec = MODEL_REGISTRY[key]\n",
    "        if cfg.tune or key not in cache:\n",
    "            print(f\"[params] Tuning: {key}\")\n",
    "            best = spec.tuner(X_raw, y, cfg)    # pass RAW X (no global scaling)\n",
    "            cache[key] = best\n",
    "            changed = True\n",
    "        out[key] = cache[key]\n",
    "\n",
    "    if changed:\n",
    "        save_cached_params(cfg.param_cache_path, cache)\n",
    "        print(f\"[params] Saved tuned params -> {cfg.param_cache_path}\")\n",
    "    else:\n",
    "        if os.path.exists(cfg.param_cache_path):\n",
    "            print(f\"[params] Loaded tuned params from cache -> {cfg.param_cache_path}\")\n",
    "        else:\n",
    "            print(\"[params] No cache file found; tuned params were computed this run.\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# TRAINING (GENERIC N-MODEL, MULTI-SEED, K-FOLD)\n",
    "# =====================================================================\n",
    "\n",
    "def train_generic_ensemble(\n",
    "    cfg: RunConfig,\n",
    "    params_by_model: Dict[str, Dict],\n",
    "    X_train_raw: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test_raw: pd.DataFrame\n",
    ") -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Train all chosen models generically with multi-seed K-Fold.\n",
    "    • For each seed:\n",
    "        - KFold splits\n",
    "        - Optionally per-fold MinMax scaling if any model needs it\n",
    "        - Train every model; collect OOF + test preds\n",
    "        - Fit meta-learner (linear/ridge) on raw OOF predictions\n",
    "        - Compare to simple uniform average; keep the better per seed\n",
    "    • Average winning predictions across seeds\n",
    "    • Train a final meta on averaged OOFs for a last comparison vs uniform avg\n",
    "    \"\"\"\n",
    "    per_seed_chosen_preds = []\n",
    "    per_seed_oof_by_model = []      # list of dicts {model: oof}\n",
    "    per_seed_test_by_model = []     # list of dicts {model: test_avg}\n",
    "    per_seed_model_rmses = []       # reporting rmse per model per seed\n",
    "\n",
    "    any_needs_scaling = cfg.needs_scaling #any(MODEL_REGISTRY[m].needs_scaling for m in cfg.models)\n",
    "\n",
    "    for seed in cfg.seeds:\n",
    "        print(f\"\\n>>> Seed {seed}\")\n",
    "        kf = KFold(n_splits=cfg.n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        # init collectors for this seed\n",
    "        oof_by_model = {m: np.zeros(len(X_train_raw)) for m in cfg.models}\n",
    "        test_by_model = {m: np.zeros(len(X_test_raw)) for m in cfg.models}\n",
    "        model_rmses = {}\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_raw), 1):\n",
    "            print(f\"    Fold {fold}/{cfg.n_folds}\")\n",
    "\n",
    "            # per-fold optional scaling (only used if any model needs it)\n",
    "            X_tr_raw = X_train_raw.iloc[tr_idx]; y_tr = y_train.iloc[tr_idx]\n",
    "            X_va_raw = X_train_raw.iloc[va_idx]; y_va = y_train.iloc[va_idx]\n",
    "            \n",
    "            if any_needs_scaling:\n",
    "                scaler = MinMaxScaler()\n",
    "                X_tr = pd.DataFrame(scaler.fit_transform(X_tr_raw), columns=X_tr_raw.columns, index=X_tr_raw.index)\n",
    "                X_va = pd.DataFrame(scaler.transform(X_va_raw), columns=X_va_raw.columns, index=X_va_raw.index)\n",
    "                X_te = pd.DataFrame(scaler.transform(X_test_raw), columns=X_test_raw.columns, index=X_test_raw.index)\n",
    "            else:\n",
    "                # tree models → raw features are fine\n",
    "                X_tr, X_va, X_te = X_tr_raw, X_va_raw, X_test_raw\n",
    "\n",
    "            # run every selected model\n",
    "            for key in cfg.models:\n",
    "                spec = MODEL_REGISTRY[key]\n",
    "                model = spec.builder(params_by_model[key], seed)\n",
    "                spec.fit_fn(model, X_tr, y_tr, X_va, y_va, cfg)\n",
    "\n",
    "                # fill OOF + accumulate test preds\n",
    "                oof_by_model[key][va_idx] = spec.predict_fn(model, X_va)\n",
    "                test_by_model[key] += spec.predict_fn(model, X_te) / cfg.n_folds\n",
    "\n",
    "        # Per-model OOF RMSEs (reporting only)\n",
    "        for key in cfg.models:\n",
    "            rmse = float(np.sqrt(mean_squared_error(y_train, oof_by_model[key])))\n",
    "            model_rmses[key] = rmse\n",
    "        print(\"    OOF RMSE by model:\", \" | \".join(f\"{k}: {v:.3f}\" for k, v in model_rmses.items()))\n",
    "\n",
    "        # Uniform average baseline (generic)\n",
    "        names = sorted(cfg.models)\n",
    "        oof_stack = np.column_stack([oof_by_model[n] for n in names])\n",
    "        test_stack = np.column_stack([test_by_model[n] for n in names])\n",
    "        uniform_oof = np.maximum(oof_stack.mean(axis=1), 0)\n",
    "        uniform_test = np.maximum(test_stack.mean(axis=1), 0)\n",
    "        uniform_rmse = float(np.sqrt(mean_squared_error(y_train, uniform_oof)))\n",
    "        print(f\"    Uniform-average OOF RMSE: {uniform_rmse:.3f}\")\n",
    "\n",
    "        # Meta-learner on raw OOFs (no engineered extras)\n",
    "        meta_model, base_order, meta_rmse = fit_meta(oof_by_model, y_train.values, cfg.meta_type, engineered=True)\n",
    "        meta_test = predict_meta(meta_model, base_order, test_by_model, engineered=True)\n",
    "        \n",
    "        print(f\"    Meta-learner ({cfg.meta_type}) OOF RMSE: {meta_rmse:.3f}\")\n",
    "\n",
    "        # pick better for this seed\n",
    "        chosen = meta_test if meta_rmse <= uniform_rmse else uniform_test\n",
    "        per_seed_chosen_preds.append(chosen)\n",
    "        per_seed_oof_by_model.append(oof_by_model)\n",
    "        per_seed_test_by_model.append(test_by_model)\n",
    "        per_seed_model_rmses.append(model_rmses)\n",
    "\n",
    "    # ===== Average across seeds =====\n",
    "    final_seed_avg = np.mean(np.column_stack(per_seed_chosen_preds), axis=1)\n",
    "\n",
    "    # Build averaged OOF across seeds for final reporting comparison\n",
    "    avg_oof_by_model: Dict[str, np.ndarray] = {k: None for k in cfg.models}\n",
    "    for d in per_seed_oof_by_model:\n",
    "        for k in cfg.models:\n",
    "            avg_oof_by_model[k] = (d[k] if avg_oof_by_model[k] is None\n",
    "                                   else avg_oof_by_model[k] + d[k])\n",
    "    for k in cfg.models:\n",
    "        avg_oof_by_model[k] /= len(cfg.seeds)\n",
    "\n",
    "    # final uniform vs final meta on averaged OOFs (reporting only)\n",
    "    names = sorted(cfg.models)\n",
    "    oof_stack = np.column_stack([avg_oof_by_model[n] for n in names])\n",
    "    uniform_oof = np.maximum(oof_stack.mean(axis=1), 0)\n",
    "    uniform_rmse = float(np.sqrt(mean_squared_error(y_train, uniform_oof)))\n",
    "    final_meta_model, col_order, meta_rmse = fit_meta(avg_oof_by_model, y_train.values, cfg.meta_type)\n",
    "    final_method = f\"Meta ({cfg.meta_type})\" if meta_rmse <= uniform_rmse else \"Uniform Average\"\n",
    "\n",
    "    report = {\n",
    "        \"per_seed_model_rmses\": per_seed_model_rmses,\n",
    "        \"final_method\": final_method,\n",
    "        \"uniform_oof_rmse\": uniform_rmse,\n",
    "        \"meta_oof_rmse\": meta_rmse\n",
    "    }\n",
    "    return np.maximum(final_seed_avg, 0), report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c21f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(argv=None) -> RunConfig:\n",
    "    \"\"\"CLI argument parsing that plays nice with Jupyter/VSCode (ignores unknown args).\"\"\"\n",
    "    p = argparse.ArgumentParser(description=\"Generic Big Mart Ensemble with N-Model Meta-Learner (Leakage-Safe)\")\n",
    "    p.add_argument(\"--train\", type=str, default=\"train_v9rqX0R.csv\", help=\"Path to train CSV\")\n",
    "    p.add_argument(\"--test\", type=str, default=\"test_AbJTz2l.csv\", help=\"Path to test CSV\")\n",
    "    p.add_argument(\"--param-cache\", type=str, default=\"tuned_params.json\", help=\"Path to param cache JSON\")\n",
    "    p.add_argument(\"--submission\", type=str, default=\"submission_generic_ensemble.csv\", help=\"Path to output submission CSV\")\n",
    "    p.add_argument(\"--seeds\", type=int, nargs=\"+\", default=None, help=\"Seeds for multi-seed averaging\")\n",
    "    p.add_argument(\"--folds\", type=int, default=10, help=\"Number of K folds\")\n",
    "    p.add_argument(\"--tune\", action=\"store_true\", help=\"Force tuning for the selected models\")\n",
    "    p.add_argument(\"--xgb-trials\", type=int, default=50, help=\"XGB Optuna trials\")\n",
    "    p.add_argument(\"--lgb-trials\", type=int, default=100, help=\"LGB Optuna trials\")\n",
    "    p.add_argument(\"--cb-trials\", type=int, default=30, help=\"CB Optuna trials\")\n",
    "    p.add_argument(\"--esr\", type=int, default=50, help=\"Early stopping rounds\")\n",
    "    p.add_argument(\"--models\", nargs=\"+\", default=None, help=\"Subset of models to use (registry keys), e.g. --models xgb lgb cb\")\n",
    "    p.add_argument(\"--meta\", type=str, choices=[\"linear\", \"ridge\"], default=\"ridge\", help=\"Meta-learner type\")\n",
    "    p.add_argument(\"--scale-policy\", choices=[True  , False],default=True)\n",
    "    \n",
    "    # <-- swallow unknown args (e.g., Jupyter's -f)\n",
    "    args, _unknown = p.parse_known_args(argv)\n",
    "\n",
    "    return RunConfig(\n",
    "        train_path=args.train,\n",
    "        test_path=args.test,\n",
    "        param_cache_path=args.param_cache,\n",
    "        submission_path=args.submission,\n",
    "        seeds=args.seeds,\n",
    "        n_folds=args.folds,\n",
    "        tune=False ,#args.tune,\n",
    "        xgb_trials=args.xgb_trials,\n",
    "        lgb_trials=args.lgb_trials,\n",
    "        cb_trials=args.cb_trials,\n",
    "        early_stopping_rounds=args.esr,\n",
    "        models=args.models,\n",
    "        meta_type=args.meta,\n",
    "        needs_scaling = True #args.scale_policy\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a6d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cfg = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a472d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "BIG MART SALES — GENERIC ENSEMBLE (N-Models + N-Pred Meta-Learner)\n",
      "========================================================================\n",
      "\n",
      "[1/6] Loading data...\n",
      "Train: (8523, 12), Test: (5681, 11)\n",
      "\n",
      "[2/6] Cleaning + feature engineering...\n",
      "\n",
      "[3/6] Preparing matrices (raw features; no global scaling)...\n",
      "\n",
      "[4/6] Load/Tune per-model hyperparameters (leakage-safe)...\n",
      "[params] Loaded tuned params from cache -> tuned_params.json\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# MAIN ORCHESTRATION\n",
    "# =====================================================================\n",
    "\n",
    "\"\"\"Orchestrate the full, generic pipeline.\"\"\"\n",
    "print(\"=\" * 72)\n",
    "print(\"BIG MART SALES — GENERIC ENSEMBLE (N-Models + N-Pred Meta-Learner)\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "print(\"\\n[1/6] Loading data...\")\n",
    "train, test, id_frame = load_data(cfg)\n",
    "print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
    "\n",
    "print(\"\\n[2/6] Cleaning + feature engineering...\")\n",
    "train_final, test_final, feature_cols = clean_and_engineer_features(train, test)\n",
    "\n",
    "print(\"\\n[3/6] Preparing matrices (raw features; no global scaling)...\")\n",
    "X_train_raw = train_final[feature_cols].fillna(0)\n",
    "X_test_raw  = test_final[feature_cols].fillna(0)\n",
    "y_train = train_final['Item_Outlet_Sales']\n",
    "\n",
    "print(\"\\n[4/6] Load/Tune per-model hyperparameters (leakage-safe)...\")\n",
    "params_by_model = get_params_for_models(cfg, X_train_raw, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ace19ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] Train generic multi-seed K-Fold ensemble...\n",
      "\n",
      ">>> Seed 42\n",
      "    Fold 1/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[294]\tvalid_0's tweedie: 311.326\n",
      "    Fold 2/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid_0's tweedie: 305.912\n",
      "    Fold 3/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid_0's tweedie: 315.753\n",
      "    Fold 4/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[261]\tvalid_0's tweedie: 309.968\n",
      "    Fold 5/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid_0's tweedie: 320.315\n",
      "    Fold 6/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[226]\tvalid_0's tweedie: 308.883\n",
      "    Fold 7/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's tweedie: 324.006\n",
      "    Fold 8/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's tweedie: 319.052\n",
      "    Fold 9/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid_0's tweedie: 319.6\n",
      "    Fold 10/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid_0's tweedie: 327.682\n",
      "    OOF RMSE by model: xgb: 1076.489 | lgb: 1074.604 | cb: 1077.185\n",
      "    Uniform-average OOF RMSE: 1074.963\n",
      "    Meta-learner (ridge) OOF RMSE: 1072.538\n",
      "\n",
      ">>> Seed 2021\n",
      "    Fold 1/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[184]\tvalid_0's tweedie: 319.766\n",
      "    Fold 2/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[207]\tvalid_0's tweedie: 320.054\n",
      "    Fold 3/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[516]\tvalid_0's tweedie: 324.951\n",
      "    Fold 4/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's tweedie: 317.975\n",
      "    Fold 5/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[207]\tvalid_0's tweedie: 309.339\n",
      "    Fold 6/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's tweedie: 315.558\n",
      "    Fold 7/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid_0's tweedie: 309.583\n",
      "    Fold 8/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid_0's tweedie: 319.665\n",
      "    Fold 9/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid_0's tweedie: 313.328\n",
      "    Fold 10/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid_0's tweedie: 312.161\n",
      "    OOF RMSE by model: xgb: 1077.425 | lgb: 1073.683 | cb: 1076.980\n",
      "    Uniform-average OOF RMSE: 1074.940\n",
      "    Meta-learner (ridge) OOF RMSE: 1071.256\n",
      "\n",
      ">>> Seed 7\n",
      "    Fold 1/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[187]\tvalid_0's tweedie: 318.773\n",
      "    Fold 2/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid_0's tweedie: 309.311\n",
      "    Fold 3/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid_0's tweedie: 316.325\n",
      "    Fold 4/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[226]\tvalid_0's tweedie: 316.765\n",
      "    Fold 5/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid_0's tweedie: 322.882\n",
      "    Fold 6/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[217]\tvalid_0's tweedie: 309.322\n",
      "    Fold 7/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid_0's tweedie: 317.07\n",
      "    Fold 8/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[369]\tvalid_0's tweedie: 316.791\n",
      "    Fold 9/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[326]\tvalid_0's tweedie: 320.867\n",
      "    Fold 10/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid_0's tweedie: 314.422\n",
      "    OOF RMSE by model: xgb: 1077.221 | lgb: 1075.106 | cb: 1079.216\n",
      "    Uniform-average OOF RMSE: 1076.032\n",
      "    Meta-learner (ridge) OOF RMSE: 1073.121\n",
      "\n",
      ">>> Seed 1337\n",
      "    Fold 1/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[419]\tvalid_0's tweedie: 316.809\n",
      "    Fold 2/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[253]\tvalid_0's tweedie: 305.256\n",
      "    Fold 3/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's tweedie: 321.863\n",
      "    Fold 4/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid_0's tweedie: 321.844\n",
      "    Fold 5/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalid_0's tweedie: 311.056\n",
      "    Fold 6/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[209]\tvalid_0's tweedie: 317.758\n",
      "    Fold 7/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's tweedie: 315.19\n",
      "    Fold 8/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid_0's tweedie: 314.411\n",
      "    Fold 9/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid_0's tweedie: 320.106\n",
      "    Fold 10/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[233]\tvalid_0's tweedie: 318.339\n",
      "    OOF RMSE by model: xgb: 1079.597 | lgb: 1075.721 | cb: 1079.958\n",
      "    Uniform-average OOF RMSE: 1077.342\n",
      "    Meta-learner (ridge) OOF RMSE: 1072.970\n",
      "\n",
      ">>> Seed 2025\n",
      "    Fold 1/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid_0's tweedie: 323.629\n",
      "    Fold 2/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[246]\tvalid_0's tweedie: 307.943\n",
      "    Fold 3/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid_0's tweedie: 304.789\n",
      "    Fold 4/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[243]\tvalid_0's tweedie: 314.474\n",
      "    Fold 5/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid_0's tweedie: 319.688\n",
      "    Fold 6/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid_0's tweedie: 326.474\n",
      "    Fold 7/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[144]\tvalid_0's tweedie: 317.019\n",
      "    Fold 8/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[202]\tvalid_0's tweedie: 314.813\n",
      "    Fold 9/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid_0's tweedie: 316.245\n",
      "    Fold 10/10\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid_0's tweedie: 317.384\n",
      "    OOF RMSE by model: xgb: 1075.496 | lgb: 1074.060 | cb: 1077.235\n",
      "    Uniform-average OOF RMSE: 1074.169\n",
      "    Meta-learner (ridge) OOF RMSE: 1070.964\n",
      "\n",
      "[6/6] Write submission...\n",
      "\n",
      "========================================================================\n",
      "SUBMISSION GENERATED\n",
      "========================================================================\n",
      "File: submission_generic_ensemble.csv\n",
      "Models used: ['xgb', 'lgb', 'cb']\n",
      "Meta type: ridge | Final method chosen: Meta (ridge)\n",
      "Uniform OOF RMSE: 1074.951 | Meta OOF RMSE: 1073.407\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/6] Train generic multi-seed K-Fold ensemble...\")\n",
    "final_preds, report = train_generic_ensemble(cfg, params_by_model, X_train_raw, y_train, X_test_raw)\n",
    "\n",
    "print(\"\\n[6/6] Write submission...\")\n",
    "submission = pd.DataFrame({\n",
    "    'Item_Identifier': id_frame['Item_Identifier'],\n",
    "    'Outlet_Identifier': id_frame['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': final_preds\n",
    "})\n",
    "submission.to_csv(cfg.submission_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"SUBMISSION GENERATED\")\n",
    "print(\"=\" * 72)\n",
    "print(f\"File: {cfg.submission_path}\")\n",
    "print(f\"Models used: {cfg.models}\")\n",
    "print(f\"Meta type: {cfg.meta_type} | Final method chosen: {report['final_method']}\")\n",
    "print(f\"Uniform OOF RMSE: {report['uniform_oof_rmse']:.3f} | Meta OOF RMSE: {report['meta_oof_rmse']:.3f}\")\n",
    "print(\"=\" * 72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377be1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b78c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
